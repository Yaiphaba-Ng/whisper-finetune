{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "145fb5a0",
   "metadata": {},
   "source": [
    "# Whisper Model Evaluation & Testing Notebook\n",
    "\n",
    "This Colab notebook allows you to load your fine-tuned Whisper model and evaluate or test it in multiple modes:\n",
    "- Evaluate on test set samples\n",
    "- Evaluate with user-provided audio/text files (Gradio)\n",
    "- Test with audio file (Gradio)\n",
    "- Test with live microphone audio\n",
    "\n",
    "It also supports language selection (auto/manual) and displays reference vs hypothesis with WER calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ead7e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import torch\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "import gradio as gr\n",
    "import jiwer\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List\n",
    "import IPython.display as ipd\n",
    "\n",
    "# For microphone input\n",
    "try:\n",
    "    import sounddevice as sd\n",
    "except ImportError:\n",
    "    sd = None\n",
    "\n",
    "# For widgets\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "except ImportError:\n",
    "    widgets = None\n",
    "    display = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031f33ae",
   "metadata": {},
   "source": [
    "## Load Trained Whisper Model\n",
    "\n",
    "Load your fine-tuned Whisper model from a local directory or Hugging Face Hub. Specify the model path or repo ID below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b08906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Log in to the Hugging Face Hub with your token.\n",
    "try:\n",
    "    login(token=\"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\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during Hugging Face login: {e}\")\n",
    "    print(\"Please double-check your hardcoded token.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc16f48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing files in ./checkpoints/whisper-medium-assamese/checkpoint-4000/: ['tokenizer.json', 'special_tokens_map.json', 'tokenizer_config.json', 'vocab.json', 'merges.txt']\n",
      "Downloading from Hugging Face Hub: openai/whisper-medium ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31767410c94e46358341a6891e1b1603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b255447d5840269802fa23ce04b7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8936a55b7484b208f2691751eab102d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aaf13875a7a45479f101a89bcd44bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f640471fe084ccebb0463e779fa0ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239857d724594c7c8635e58d1a650889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998251cd1d07412ba51193027e125291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied tokenizer.json\n",
      "Copied special_tokens_map.json\n",
      "Copied tokenizer_config.json\n",
      "Copied vocab.json\n",
      "Copied merges.txt\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "MODEL_PATH = \"./checkpoints/whisper-medium-assamese/checkpoint-4000/\"  # Your trained model dir\n",
    "BASE_MODEL_REPO = \"openai/whisper-medium\"  # Change if you used a different base model\n",
    "\n",
    "# List of files typically needed for Whisper processor/tokenizer\n",
    "required_files = [\n",
    "    \"preprocessor_config.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"special_tokens_map.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"vocab.json\",\n",
    "    \"merges.txt\"\n",
    "]\n",
    "\n",
    "missing = []\n",
    "for fname in required_files:\n",
    "    if not os.path.exists(os.path.join(MODEL_PATH, fname)):\n",
    "        missing.append(fname)\n",
    "\n",
    "if missing:\n",
    "    print(f\"Missing files in {MODEL_PATH}: {missing}\")\n",
    "    print(f\"Downloading from Hugging Face Hub: {BASE_MODEL_REPO} ...\")\n",
    "    # Download snapshot to a temporary directory\n",
    "    base_model_dir = snapshot_download(BASE_MODEL_REPO, allow_patterns=required_files)\n",
    "    for fname in missing:\n",
    "        src = os.path.join(base_model_dir, fname)\n",
    "        dst = os.path.join(MODEL_PATH, fname)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "            print(f\"Copied {fname}\")\n",
    "        else:\n",
    "            print(f\"WARNING: {fname} not found in {BASE_MODEL_REPO}, skipping.\")\n",
    "else:\n",
    "    print(\"All processor/tokenizer files present in model directory.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f9f6511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./checkpoints/whisper-medium-assamese/checkpoint-4000/ on cpu\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to your trained model (local directory or Hugging Face repo ID)\n",
    "MODEL_PATH = \"./checkpoints/whisper-medium-assamese/checkpoint-4000/\"  # Change as needed\n",
    "\n",
    "# Load processor and model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_PATH)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_PATH).to(device)\n",
    "model.eval()\n",
    "print(f\"Model loaded from {MODEL_PATH} on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c13ef1",
   "metadata": {},
   "source": [
    "## Language Selection (Auto/Manual)\n",
    "\n",
    "Choose whether to use automatic language detection or manually specify the language for transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1118dffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2201d48ac343fea6ea25cfa40eeb92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Language Mode:', options=(('Auto', 'auto'), ('Manual', 'manual')), value='auto')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121c1eca811e43719bbbf9b21d1c33cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='as', description='Language:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Language selection widget (auto/manual)\n",
    "language_mode = \"auto\"  # or \"manual\"\n",
    "manual_language = \"as\"   # default ISO code for Assamese\n",
    "\n",
    "if widgets:\n",
    "    language_mode_widget = widgets.ToggleButtons(\n",
    "        options=[('Auto', 'auto'), ('Manual', 'manual')],\n",
    "        value='auto',\n",
    "        description='Language Mode:',\n",
    "        disabled=False,\n",
    "        button_style=''\n",
    "    )\n",
    "    manual_language_widget = widgets.Text(\n",
    "        value='as',\n",
    "        description='Language:',\n",
    "        disabled=False\n",
    "    )\n",
    "    def on_language_mode_change(change):\n",
    "        global language_mode\n",
    "        language_mode = change['new']\n",
    "        if language_mode == 'manual':\n",
    "            display(manual_language_widget)\n",
    "    language_mode_widget.observe(on_language_mode_change, names='value')\n",
    "    display(language_mode_widget)\n",
    "else:\n",
    "    print(\"ipywidgets not available. Set language_mode and manual_language variables manually.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66716cf4",
   "metadata": {},
   "source": [
    "## Mode Selection\n",
    "\n",
    "Select the mode for evaluation or testing: \n",
    "- Eval on test set\n",
    "- Eval with user files (Gradio)\n",
    "- Test with audio file (Gradio)\n",
    "- Test with live mic audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1e26a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1df1766a1cd47c2840d11fdee5ddb38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Mode:', options=(('Eval: Test Set Samples', 'eval_testset'), ('Eval: User Audio/Tex…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mode selection widget\n",
    "eval_modes = [\n",
    "    (\"Eval: Test Set Samples\", \"eval_testset\"),\n",
    "    (\"Eval: User Audio/Text Files (Gradio)\", \"eval_user_files\"),\n",
    "    (\"Test: Transcribe Audio File (Gradio)\", \"test_audio_file\"),\n",
    "    (\"Test: Live Mic Transcription\", \"test_live_mic\")\n",
    "]\n",
    "selected_mode = \"eval_testset\"\n",
    "\n",
    "if widgets:\n",
    "    mode_widget = widgets.ToggleButtons(\n",
    "        options=eval_modes,\n",
    "        value='eval_testset',\n",
    "        description='Mode:',\n",
    "        disabled=False,\n",
    "        button_style=''\n",
    "    )\n",
    "    display(mode_widget)\n",
    "else:\n",
    "    print(\"ipywidgets not available. Set selected_mode variable manually.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde8a87b",
   "metadata": {},
   "source": [
    "## Display Reference vs Hypothesis and Calculate WER\n",
    "\n",
    "For all evaluation modes, display the reference and hypothesis transcriptions side by side and calculate the Word Error Rate (WER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64a7ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to display reference vs hypothesis and calculate WER\n",
    "def display_ref_vs_hyp(refs: List[str], hyps: List[str]):\n",
    "    for i, (ref, hyp) in enumerate(zip(refs, hyps)):\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(f\"Reference:   {ref}\")\n",
    "        print(f\"Hypothesis:  {hyp}\")\n",
    "        print(\"-\")\n",
    "    wer = jiwer.wer(refs, hyps)\n",
    "    print(f\"WER: {wer:.3f}\")\n",
    "\n",
    "# Example usage (after running an eval mode):\n",
    "# display_ref_vs_hyp(references, hypotheses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b22971",
   "metadata": {},
   "source": [
    "## Eval Mode: Evaluate on Test Set Samples\n",
    "\n",
    "Select a number of samples from the test set, run inference, and calculate WER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ac5372e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lang' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transcription\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Load test set (update as needed)\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m testset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmozilla-foundation/common_voice_11_0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mlang\u001b[49m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./datasets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Change as needed\u001b[39;00m\n\u001b[0;32m     20\u001b[0m samples \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mlist\u001b[39m(testset), num_samples)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lang' is not defined"
     ]
    }
   ],
   "source": [
    "# Example: Load test set and evaluate a few samples\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "def transcribe(audio, language=None):\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    if language_mode == 'manual' and manual_language:\n",
    "        forced_lang = manual_language\n",
    "    else:\n",
    "        forced_lang = None\n",
    "    input_features = inputs.input_features.to(device)\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features, language=forced_lang)\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    return transcription\n",
    "\n",
    "# Load test set (update as needed)\n",
    "testset = load_dataset(\"mozilla-foundation/common_voice_11_0\", lang, split=\"test\", cache_dir=\"./datasets\")\n",
    "num_samples = 5  # Change as needed\n",
    "samples = random.sample(list(testset), num_samples)\n",
    "\n",
    "references = []\n",
    "hypotheses = []\n",
    "for sample in samples:\n",
    "    audio = sample['audio']['array']\n",
    "    ref = sample['sentence']\n",
    "    hyp = transcribe(audio)\n",
    "    references.append(ref)\n",
    "    hypotheses.append(hyp)\n",
    "    print(f\"Reference: {ref}\\nHypothesis: {hyp}\\n---\")\n",
    "\n",
    "# Calculate WER and display results\n",
    "wer = jiwer.wer(references, hypotheses)\n",
    "print(f\"WER on {num_samples} test samples: {wer:.3f}\")\n",
    "display_ref_vs_hyp(references, hypotheses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4492997",
   "metadata": {},
   "source": [
    "## Eval Mode: User Provided Audio and Text Files (Gradio)\n",
    "\n",
    "Upload audio and reference text files for evaluation. The notebook will transcribe the audio, display reference vs hypothesis, and calculate WER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9795a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio interface for user-provided audio and text files\n",
    "def eval_user_files(audio_file, text_file):\n",
    "    audio, sr = librosa.load(audio_file, sr=16000)\n",
    "    with open(text_file, 'r', encoding='utf-8') as f:\n",
    "        reference = f.read().strip()\n",
    "    hypothesis = transcribe(audio)\n",
    "    # Use display_ref_vs_hyp for consistent output\n",
    "    import io\n",
    "    import sys\n",
    "    buf = io.StringIO()\n",
    "    sys.stdout = buf\n",
    "    display_ref_vs_hyp([reference], [hypothesis])\n",
    "    sys.stdout = sys.__stdout__\n",
    "    return buf.getvalue()\n",
    "\n",
    "gr.Interface(\n",
    "    fn=eval_user_files,\n",
    "    inputs=[gr.Audio(source=\"upload\", type=\"filepath\", label=\"Audio File\"), gr.File(label=\"Reference Text File\")],\n",
    "    outputs=\"text\",\n",
    "    title=\"Eval: User Provided Audio and Text Files\",\n",
    "    description=\"Upload an audio file and a reference text file to evaluate WER.\"\n",
    ").launch(share=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bb7f73",
   "metadata": {},
   "source": [
    "## Test Mode: Transcribe from Audio File (Gradio)\n",
    "\n",
    "Upload an audio file to transcribe using the trained Whisper model. The transcription will be displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1336c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio interface for audio file transcription\n",
    "def transcribe_audio_file(audio_file):\n",
    "    audio, sr = librosa.load(audio_file, sr=16000)\n",
    "    transcription = transcribe(audio)\n",
    "    return transcription\n",
    "\n",
    "gr.Interface(\n",
    "    fn=transcribe_audio_file,\n",
    "    inputs=gr.Audio(source=\"upload\", type=\"filepath\", label=\"Audio File\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Test: Transcribe Audio File\",\n",
    "    description=\"Upload an audio file to transcribe using the trained Whisper model.\"\n",
    ").launch(share=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6953f07c",
   "metadata": {},
   "source": [
    "## Test Mode: Live Transcription from Microphone\n",
    "\n",
    "Capture audio from your microphone and transcribe it in real time using the trained Whisper model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467213ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live microphone transcription (requires sounddevice)\n",
    "def record_and_transcribe(duration=5, fs=16000):\n",
    "    if sd is None:\n",
    "        return \"sounddevice not installed.\"\n",
    "    print(f\"Recording {duration} seconds of audio...\")\n",
    "    audio = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='float32')\n",
    "    sd.wait()\n",
    "    audio = audio.flatten()\n",
    "    transcription = transcribe(audio)\n",
    "    print(f\"Transcription: {transcription}\")\n",
    "    return transcription\n",
    "\n",
    "if sd is not None:\n",
    "    duration = 5  # seconds\n",
    "    print(\"Click in the cell and run to record from your mic.\")\n",
    "    record_and_transcribe(duration=duration)\n",
    "else:\n",
    "    print(\"sounddevice not available. Install it to use live mic transcription.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
